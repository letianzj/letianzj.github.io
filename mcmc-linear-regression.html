<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="bokhEjod5-cme1ZueY6aAPHORfnXv9hSoljSfg2Vp_Q">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"letianzj.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Introduction In last post we examined the Bayesian approach for linear regression. It relies on the conjugate prior assumption, which nicely sets posterior to Gaussian distribution. In reality, most">
<meta property="og:type" content="article">
<meta property="og:title" content="MCMC Linear Regression">
<meta property="og:url" content="https://letianzj.github.io/mcmc-linear-regression.html">
<meta property="og:site_name" content="Systematic Investing">
<meta property="og:description" content="Introduction In last post we examined the Bayesian approach for linear regression. It relies on the conjugate prior assumption, which nicely sets posterior to Gaussian distribution. In reality, most">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://letianzj.github.io/posterior_distribution.png">
<meta property="og:image" content="https://letianzj.github.io/mcmc-linear-regression/posterior_distribution.png">
<meta property="article:published_time" content="2017-12-31T02:30:00.000Z">
<meta property="article:modified_time" content="2018-10-16T00:27:16.336Z">
<meta property="article:author" content="Letian Wang">
<meta property="article:tag" content="MCMC">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Linear Regression">
<meta property="article:tag" content="Markov Chain Monte Carlo">
<meta property="article:tag" content="Bayesian Inference">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://letianzj.github.io/posterior_distribution.png">

<link rel="canonical" href="https://letianzj.github.io/mcmc-linear-regression.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>MCMC Linear Regression | Systematic Investing</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-109341958-2"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-109341958-2');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Systematic Investing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Quant Trading and Portfolio Management</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://letianzj.github.io/mcmc-linear-regression.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Letian Wang">
      <meta itemprop="description" content="Letian Wang quantitative trading blog, discuss topics includding trading strategies, portfolio management, risk premia, risk management, systematic trading, and machine learning, deep learning applications in Finance.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Systematic Investing">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MCMC Linear Regression
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-12-30 21:30:00" itemprop="dateCreated datePublished" datetime="2017-12-30T21:30:00-05:00">2017-12-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2018-10-15 20:27:16" itemprop="dateModified" datetime="2018-10-15T20:27:16-04:00">2018-10-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/mcmc-linear-regression.html#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="mcmc-linear-regression.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="introduction">Introduction</h2>
<p>In <a href="/bayesian-linear-regression.html" title="last post">last post</a> we examined the Bayesian approach for linear regression. It relies on the <a href="https://en.wikipedia.org/wiki/Conjugate_prior" target="_blank" rel="noopener">conjugate prior</a> assumption, which nicely sets posterior to Gaussian distribution. In reality, most times we don't have this luxury, so we rely instead on a technique called <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" target="_blank" rel="noopener">Markov Chain Monte Carlo (MCMC)</a>. One popular algorithm in this family is <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" target="_blank" rel="noopener">Metropolis–Hastings</a> and this is what we are looking at today. Before we proceed, I want to point out that this post is inpsired by <a href="https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/" target="_blank" rel="noopener">this article in R</a>.</p>
<p>MCMC answers to this question: if we don't know what the posterior distribution looks like, and we don't have the closed form solution as given in equation (2.5) of <a href="/bayesian-linear-regression.html" title="last post">last post</a> for <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\Sigma_{\beta,1}\)</span>, how do we obtain the posterior distrubtion of <span class="math inline">\(\beta\)</span>? Can we at least approximate it? Metropolis–Hastings provides a numerical Monte Carlo simulation method to magically draw a sample out of the posterior distribution. The magic is to construct a Markov Chain that converges to the given distribution as its stationary equilibrium distribution. Hence the name Markov Chain Monte Carlo (MCMC).</p>
<a id="more"></a>
<h2 id="mcmc-simple-linear-regression">MCMC Simple Linear Regression</h2>
<p>Back to our simple linear regression example, firstly let's repeat the Bayesian Theorem from <a href="/bayesian-linear-regression.html" title="last post">last post</a>.</p>
<p><span class="math display">\[
f(\beta|y,X)=\frac{f(y|\beta,X)f(\beta|X)}{f(y|X)}
\tag{2.1}
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is regression parameter; y and X each have 500 observations.</p>
<p>The distribution of interest is our posterior distribution of <span class="math inline">\(\beta\)</span> denoted as <span class="math inline">\(f(\beta|y,X)\)</span>. This time we technically only do one Bayesian iteration (remember we updated our belief 250 times in the last post?) because the purpose is to show how to draw a MCMC sample distribution from the first posterior iteration. We include all the 500 points in the Bayesian so the result is expected to reflect full information dataset and converge to the true value.</p>
<p>The Metropolis-Hastings algorithm works as follows.</p>
<ol type="1">
<li>Start with a random parameter value <span class="math inline">\(\beta_0\)</span></li>
<li>Choose a new <span class="math inline">\(\beta&#39;\)</span> based on some proposal function <span class="math inline">\(g(\beta&#39;|\beta_0)\)</span>. As pointed out <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" target="_blank" rel="noopener">here</a>, <span class="math inline">\(g\)</span> must be symmetric, usually just Gaussian.</li>
<li>Calculate the acceptance ratio</li>
</ol>
<p><span class="math display">\[
\alpha=\frac{f(y|\beta&#39;)f(\beta&#39;)}{f(y|\beta_0)f(\beta_0)}  \tag{2.2}
\]</span></p>
<ol start="4" type="1">
<li>Draw a random number <span class="math inline">\(u \in [0,1]\)</span>. Jump from <span class="math inline">\(\beta_0\)</span> to <span class="math inline">\(\beta&#39;\)</span> if <span class="math inline">\(u &lt;= \alpha\)</span>, and denote it as <span class="math inline">\(\beta_1\)</span>; otherwise stay with the old point.</li>
<li>Repeat step 2, 3, 4, and collect <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>,...,etc.</li>
</ol>
<p>In the end, throw away some initial <span class="math inline">\(\beta\)</span> values in the beginning based on burn-in hyper-parameter, and Metropolis-Hastings claims that the remaining <span class="math inline">\(\beta\)</span> series comes from the distribution of posterior <span class="math inline">\(f(\beta|y,X)\)</span>.</p>
<p>Now let's interpret this process by code. There are Python packages such as <a href="https://docs.pymc.io/" target="_blank" rel="noopener">PyMC3</a> to use out-of-box but sometimes it helps to look into the black box.</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.stats</span><br><span class="line"><span class="comment"># don't forget to generate the 500 random samples as in the previous post</span></span><br><span class="line">sigma_e = <span class="number">3.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Similar to last post, let's initially believe that a, b follow Normal distribution with mean 0.5 and stadndard deviation 0.5</span></span><br><span class="line"><span class="comment"># it returns the probability of seeing beta under this belief</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prior_probability</span><span class="params">(beta)</span>:</span></span><br><span class="line">    a = beta[<span class="number">0</span>]     <span class="comment"># intercept</span></span><br><span class="line">    b = beta[<span class="number">1</span>]     <span class="comment"># slope</span></span><br><span class="line">    a_prior = scipy.stats.norm(<span class="number">0.5</span>, <span class="number">0.5</span>).pdf(a)</span><br><span class="line">    b_prior = scipy.stats.norm(<span class="number">0.5</span>, <span class="number">0.5</span>).pdf(b)</span><br><span class="line">    <span class="comment"># log probability transforms multiplication to summation</span></span><br><span class="line">    <span class="keyword">return</span> np.log(a) + np.log(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Given beta, the likehood of seeing x and y</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">likelihood_probability</span><span class="params">(beta)</span>:</span></span><br><span class="line">    a = beta[<span class="number">0</span>]     <span class="comment"># intercept</span></span><br><span class="line">    b = beta[<span class="number">1</span>]     <span class="comment"># slope</span></span><br><span class="line">    y_predict = a  + b * x</span><br><span class="line">    single_likelihoods = scipy.stats.norm(y_predict, sigma_e).pdf(y)        <span class="comment"># we know sigma_e is 3.0</span></span><br><span class="line">    <span class="keyword">return</span> np.sum(np.log(single_likelihoods))</span><br><span class="line"></span><br><span class="line"><span class="comment"># We don't need to know the denominator of support f(y)</span></span><br><span class="line"><span class="comment"># as it will be canceled out in the acceptance ratio</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">posterior_probability</span><span class="params">(beta)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> likelihood_probability(beta) + prior_probability(beta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># jump from beta to new beta</span></span><br><span class="line"><span class="comment"># proposal function is Gaussian centered at beta</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">proposal_function</span><span class="params">(beta)</span>:</span></span><br><span class="line">    a = beta[<span class="number">0</span>]</span><br><span class="line">    b = beta[<span class="number">1</span>]</span><br><span class="line">    a_new = np.random.normal(a, <span class="number">0.5</span>)</span><br><span class="line">    b_new = np.random.normal(b, <span class="number">0.5</span>)</span><br><span class="line">    beta_new = [a_new, b_new]</span><br><span class="line">    <span class="keyword">return</span> beta_new</span><br><span class="line"></span><br><span class="line"><span class="comment"># run the Monte Carlo</span></span><br><span class="line">beta_0 = [<span class="number">0.5</span>, <span class="number">0.5</span>]        <span class="comment"># start value</span></span><br><span class="line">results = np.zeros([<span class="number">50000</span>,<span class="number">2</span>])            <span class="comment"># record the results</span></span><br><span class="line">results[<span class="number">0</span>,<span class="number">0</span>] = beta_0[<span class="number">0</span>]</span><br><span class="line">results[<span class="number">0</span>, <span class="number">1</span>] = beta_0[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">50000</span>):               <span class="comment"># loop 50,000 times</span></span><br><span class="line">    print(<span class="string">'step: &#123;&#125;'</span>.format(step))</span><br><span class="line"></span><br><span class="line">    beta_old = results[step<span class="number">-1</span>, :]</span><br><span class="line">    beta_proposal = proposal_function(beta_old)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use np.exp to restore from log numbers</span></span><br><span class="line">    prob = np.exp(posterior_probability(beta_proposal) - posterior_probability(beta_old))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.random.uniform(<span class="number">0</span>,<span class="number">1</span>) &lt; prob:</span><br><span class="line">        results[step, :] = beta_proposal    <span class="comment"># jump</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        results[step, :] = beta_old         <span class="comment"># stay</span></span><br><span class="line"></span><br><span class="line">burn_in = <span class="number">10000</span></span><br><span class="line">beta_posterior = results[burn_in:, :]</span><br><span class="line">print(beta_posterior.mean(axis=<span class="number">0</span>))        <span class="comment"># use average as point estimates</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># present the results</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">121</span>)</span><br><span class="line">ax1.hist(beta_posterior[:,<span class="number">0</span>], bins=<span class="number">20</span>, color=<span class="string">'blue'</span>)</span><br><span class="line">ax1.axvline(beta_posterior.mean(axis=<span class="number">0</span>)[<span class="number">0</span>], color=<span class="string">'red'</span>, linestyle=<span class="string">'dashed'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">ax1.title.set_text(<span class="string">'Posterior -- Intercept'</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">122</span>)</span><br><span class="line">ax2.hist(beta_posterior[:,<span class="number">1</span>], bins=<span class="number">20</span>, color=<span class="string">'blue'</span>)</span><br><span class="line">ax2.axvline(beta_posterior.mean(axis=<span class="number">0</span>)[<span class="number">1</span>], color=<span class="string">'red'</span>, linestyle=<span class="string">'dashed'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">ax2.title.set_text(<span class="string">'Posterior -- Slope'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>Here is the posterior distribution of <span class="math inline">\(\beta\)</span>. The intercept converges to 0.75 (linear regress gives 0.6565181) and the slope converges to 2 (linear regression gives 2.0086851). MCMC does the job.</p>
<!--<img src="posterior_distribution.png" align="left" width="100%" height="100%">-->
<img src="/mcmc-linear-regression/posterior_distribution.png" class="" title="Posterior Distribution">
<p><strong>Reference</strong> * <a href="https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/" target="_blank" rel="noopener">Metropolis Hastings MCMC in R, 2010</a> * <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" target="_blank" rel="noopener">Metropolis Hastings Algorithm, Wikipedia</a></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Letian Wang
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://letianzj.github.io/mcmc-linear-regression.html" title="MCMC Linear Regression">https://letianzj.github.io/mcmc-linear-regression.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/bayesian-linear-regression.html" rel="prev" title="Bayesian Linear Regression">
      <i class="fa fa-chevron-left"></i> Bayesian Linear Regression
    </a></div>
      <div class="post-nav-item">
    <a href="/kalman-filter-linear-regression.html" rel="next" title="Kalman Filter Linear Regression">
      Kalman Filter Linear Regression <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mcmc-simple-linear-regression"><span class="nav-number">2.</span> <span class="nav-text">MCMC Simple Linear Regression</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Letian Wang"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Letian Wang</p>
  <div class="site-description" itemprop="description">Letian Wang quantitative trading blog, discuss topics includding trading strategies, portfolio management, risk premia, risk management, systematic trading, and machine learning, deep learning applications in Finance.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/letianzj" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;letianzj" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:letian.zj@gmail.com" title="E-Mail → mailto:letian.zj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/letian-wang-phd-cfa-frm-75b53312/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;letian-wang-phd-cfa-frm-75b53312&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Letian Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-letianzj-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://letianzj.github.io/mcmc-linear-regression.html";
    this.page.identifier = "mcmc-linear-regression.html";
    this.page.title = "MCMC Linear Regression";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://https-letianzj-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
