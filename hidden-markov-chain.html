<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hidden Markov Chain,Machine Learning,Regime Detection,Backward Algorithm,Forward Algorithm,Viterbi Algorithm,Baum-Welch Algorithm" />










<meta name="description" content="IntroductionHidden Markov Model (HMM) is a Markov Model with latent state space. It is the discrete version of Dynamic Linear Model, commonly seen in speech recognition. In quantitative trading, it ha">
<meta name="keywords" content="Hidden Markov Chain,Machine Learning,Regime Detection,Backward Algorithm,Forward Algorithm,Viterbi Algorithm,Baum-Welch Algorithm">
<meta property="og:type" content="article">
<meta property="og:title" content="Hidden Markov Chain">
<meta property="og:url" content="https://letianzj.github.io/hidden-markov-chain.html">
<meta property="og:site_name" content="Systematic Investment">
<meta property="og:description" content="IntroductionHidden Markov Model (HMM) is a Markov Model with latent state space. It is the discrete version of Dynamic Linear Model, commonly seen in speech recognition. In quantitative trading, it ha">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://letianzj.github.io/hidden-markov-chain/example.png">
<meta property="og:image" content="https://letianzj.github.io/hidden-markov-chain/spx_hmm.png">
<meta property="og:updated_time" content="2018-10-16T03:05:52.833Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hidden Markov Chain">
<meta name="twitter:description" content="IntroductionHidden Markov Model (HMM) is a Markov Model with latent state space. It is the discrete version of Dynamic Linear Model, commonly seen in speech recognition. In quantitative trading, it ha">
<meta name="twitter:image" content="https://letianzj.github.io/hidden-markov-chain/example.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://letianzj.github.io/hidden-markov-chain.html"/>





  <title>Hidden Markov Chain | Hidden Markov Chain,Machine Learning,Regime Detection,Backward Algorithm,Forward Algorithm,Viterbi Algorithm,Baum-Welch Algorithm | Systematic Investment</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-109341958-2', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Systematic Investment</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Quant Trading and Portfolio Management</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://letianzj.github.io/hidden-markov-chain.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Letian Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Systematic Investment">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Hidden Markov Chain</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-02T07:05:00-04:00">
                2018-09-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/hidden-markov-chain.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="hidden-markov-chain.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
		  
		  

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" target="_blank" rel="noopener">Hidden Markov Model (HMM)</a> is a <a href="https://en.wikipedia.org/wiki/Markov_model" target="_blank" rel="noopener">Markov Model</a> with latent <a href="https://en.wikipedia.org/wiki/State_space" target="_blank" rel="noopener">state space</a>. It is the discrete version of Dynamic Linear Model, commonly seen in speech recognition. In quantitative trading, it has been applied to detecting latent market regimes ([2], [3]). I’ll relegate technical details to appendix and present the intuitions by an example.</p>
<a id="more"></a>
<!--<img src="example.png" align="left" width="100%" height="100%">-->
<img src="/hidden-markov-chain/example.png" title="Example">
<p>No cell phone, no TV, without logging into Bloomberg terminal, this simple example teaches you how to guess market movements solely from the trader’s mood. Here the two hidden states are market rally or drop. The transition matrix stipulates that if the market goes up today, there are 80% chance that the trend will continue tomorrow and 20% chance that the trend will reverse. On the contrary, if the market drops today, there are 70% chance sliding further tomorrow and 30% chance turning back up. </p>
<p>The emission matrix shows the dependency of the trader’s mood on mareket changes. If she is just moody (high noise/information ratio), it would tell us nothing about the market. If somehow her mood is driven by her PnL, then we might be able to learn the market from her behavior. In this example, it says that if the market heads up, she would most likely be happy but there are 10% chance that something else, for example a car accident in the morning, upsets here. On the contrary, if the market goes down she would be unhappy, unless something else, perphas today happens to be her anniversary, cheers her up.</p>
<p>With all these uncertainties/probabilities in place, there are three questions to ask, in the order of complexity:</p>
<ol>
<li><p><strong>Evaluation</strong>: What is the odds that she is happy three days in a row?</p>
</li>
<li><p><strong>Decoding</strong>: Given that she is happy three days in a row, what would be the most likely market movements in the last three days?</p>
</li>
<li><p><strong>Learning</strong>: If we don’t know any of the probablities labeled next to the arrows in the figure, given that she is happy three days in a row, what would be our best estimates of these probabilities (model parameters)?</p>
</li>
</ol>
<p>The first question can be answered by <a href="https://en.wikipedia.org/wiki/Forward_algorithm" target="_blank" rel="noopener">forward</a> OR backward algorithm (do not confuse with <a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm" target="_blank" rel="noopener">forward-backward algorithm</a>). The second question is tackled by <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Viterbi algorithm</a>. The third one is solved by <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener">Baum-Welch algorithm</a>, an <a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm" target="_blank" rel="noopener">forward-backward</a> <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" target="_blank" rel="noopener">expectation maximization</a> approach.</p>
<h2 id="Regime-Detection"><a href="#Regime-Detection" class="headerlink" title="Regime Detection"></a>Regime Detection</h2><p>This section follows the <a href="https://hmmlearn.readthedocs.io/en/latest/auto_examples/plot_hmm_stock_analysis.html" target="_blank" rel="noopener">accompanying example</a> of the package <a href="https://github.com/hmmlearn/hmmlearn" target="_blank" rel="noopener">HMMLearn</a>. The code is located on <a href="https://github.com/EliteQuant/EliteQuant_Python/blob/master/research/hidden_markov_chain.py" target="_blank" rel="noopener">Github</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hmm_model = GaussianHMM(</span><br><span class="line">    n_components=<span class="number">3</span>,                     <span class="comment"># number of states</span></span><br><span class="line">    covariance_type=<span class="string">"full"</span>,             <span class="comment"># full covariance matrix vs diagonal</span></span><br><span class="line">    n_iter=<span class="number">1000</span>                         <span class="comment"># number of iterations</span></span><br><span class="line">).fit(rets)</span><br><span class="line"></span><br><span class="line">print(hmm_model.transmat_)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Means and vars of each hidden state"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(hmm_model.n_components):                   <span class="comment"># 0 is down, 1 is up</span></span><br><span class="line">    print(<span class="string">"&#123;0&#125;th hidden state"</span>.format(i))</span><br><span class="line">    print(<span class="string">"mean = "</span>, hmm_model.means_[i])</span><br><span class="line">    print(<span class="string">"var = "</span>, np.diag(hmm_model.covars_[i]))</span><br></pre></td></tr></table></figure>
<p>From the mean and standard deviation, it seems that state 0 is market-down state; state 1 and 2 are market-up states. Within them, state 3 has $65\%$ of chance flipping to the downside. Below is the historical states distribution of S&amp;P 500 index. At first glance it doesn’t seem to be a very precise result. It might need to be combined with other strategies.</p>
<img src="/hidden-markov-chain/spx_hmm.png" title="HMM States">
<p><strong>Reference</strong></p>
<ul>
<li>[1] Petris, Giovanni, Sonia Petrone, and Patrizia Campagnoli. “Dynamic linear models.” Dynamic Linear Models with R. Springer, New York, NY, 2009. 31-84.</li>
<li>[2] Nguyen, Nguyet, and Dung Nguyen. “Hidden Markov model for stock selection.” Risks 3.4 (2015): 455-473.</li>
<li>[3] Nguyen, Nguyet. “Hidden Markov Model for Stock Trading.” International Journal of Financial Studies 6.2 (2018): 36.</li>
<li>[4] Wikipeida, <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" target="_blank" rel="noopener">Hidden Markov Model</a></li>
<li>[5] Wikipeida, <a href="https://en.wikipedia.org/wiki/Forward_algorithm" target="_blank" rel="noopener">Forward algorithm</a></li>
<li>[6] Wikipedia, <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Viterbi algorithm</a></li>
<li>[7] Wikipedia, <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener">Baum–Welch algorithm</a></li>
</ul>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p>You might need some basic understanding about <a href="https://en.wikipedia.org/wiki/Markov_chain" target="_blank" rel="noopener">Markov Chain</a> in order to continue.</p>
<p>This example uses the notations on <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener">Wikipedia</a>. In particular the hidden markov system  $\theta=\left(A,B,\pi\right)$ is given by</p>
<p>$$<br>\begin{matrix}<br>A=\left[ \begin {array}{cc} 0.8 &amp; 0.2\\ 0.3 &amp; 0.7 \end{array} \right], B=\left[ \begin {array}{cc} 0.9 &amp; 0.1\\ 0.4 &amp; 0.6 \end{array} \right],\pi=\left[ \begin {array}{c} 0.5\\ 0.5 \end{array} \right]<br>\tag{A1}<br>\end{matrix}<br>$$</p>
<p>You can find these numbers from the figure above.</p>
<h3 id="First-Question-–-Evaluation"><a href="#First-Question-–-Evaluation" class="headerlink" title="First Question – Evaluation"></a>First Question – Evaluation</h3><p>The first question is, given we know $\theta$, what is the odds that she is happy three days in a row? This is answered by the forward procedure or backward procedure and is solved recursively.</p>
<table>
<thead>
<tr>
<th>Day</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>$\alpha_1(1)=0.5\times 0.9=0.45$</td>
</tr>
<tr>
<td></td>
<td>$\alpha_2(1)=0.5\times 0.4=0.2$</td>
</tr>
<tr>
<td>2</td>
<td>$\alpha_1(2)=0.9\times\left[\alpha_1(1)\times0.8+\alpha_2(1)\times0.3\right]=0.378$</td>
</tr>
<tr>
<td></td>
<td>$\alpha_2(2)=0.4\times\left[\alpha_1(1)\times0.2+\alpha_2(1)\times0.7\right]=0.092$</td>
</tr>
<tr>
<td>3</td>
<td>$\alpha_1(3)=0.9\times\left[\alpha_1(2)\times0.8+\alpha_2(2)\times0.3\right]=0.297$</td>
</tr>
<tr>
<td></td>
<td>$\alpha_2(3)=0.4\times\left[\alpha_1(2)\times0.2+\alpha_2(2)\times0.7\right]=0.056$</td>
</tr>
</tbody>
</table>
<p>On day 1, there are $45\%$ chance to observe a happy face (H) and market is up; there are $20\%$ chance to observe a happy face (H) and market is down. So in total there are $65\%$ chance that she would be happy and 35% chance that she would be sad (S). Similarly, the probabilty of seeing $HH$ is $47\%$. Other possibilities (that is, $HH$, $HS$ and $SS$) add up to $53\%$.</p>
<p>Finailly on day three, the answer to our question becomes $35.3\%$.</p>
<p>If we iterate backwards,</p>
<table>
<thead>
<tr>
<th>Day</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>$\beta_1(3)=1.0$</td>
</tr>
<tr>
<td></td>
<td>$\beta_2(3)=1.0$</td>
</tr>
<tr>
<td>2</td>
<td>$\beta_1(2)=\beta_1(2)\times0.8\times0.9+\beta_2(2)\times0.2\times0.4=0.8$</td>
</tr>
<tr>
<td></td>
<td>$\beta_2(2)=\beta_1(2)\times0.3\times0.9+\beta_2(2)\times0.7\times0.4=0.55$</td>
</tr>
<tr>
<td>3</td>
<td>$\beta_1(1)=0.9\times[\beta_1(2)\times0.8+\beta_2(2)\times0.3]=0.62$</td>
</tr>
<tr>
<td></td>
<td>$\beta_2(1)=0.4\times[\beta_1(2)\times0.2+\beta_2(2)\times0.7]=0.37$</td>
</tr>
</tbody>
</table>
<p>The final probability should match that from the forward algorithm ([2])</p>
<p>$$<br>Pr(HHH|\theta)=0.5\times0.62\times0.9+0.5\times0.37\times0.4=35.3\%<br>\tag{A2}<br>$$</p>
<h3 id="Second-Question-–-Decoding"><a href="#Second-Question-–-Decoding" class="headerlink" title="Second Question – Decoding"></a>Second Question – Decoding</h3><!--
Let define ([2])

$$
\delta_i(t)=\underset{x_1,x_2,...,x_{t-1}}{max}P\left(X_1=x_1,X_2=x_2,...,X_t=i,Y_1=y_1,...,Y_t=y_t|\theta \right)
\tag{A3}
$$

By induction, we have:

$$
\delta_j(t+1)=\underset{i}{max}[\delta_i(t)a_{ij}]b_j(y_{t+1})
\tag{A4}
$$

The most likely state $i$, at time $t$, is the one that maximize (A3):

$$
i_t=\underset{1\le i\le N}{argmax}\delta_i(t)
\tag{A5}
$$

Therefore, the Viterbi algorithm is given by

1. Initialization:

$$
\begin{matrix}
\delta_j(1) &=& p_jb_j(y_1) \\\\
\xi_j(1) &=& 0
\end{matrix}
\tag{A6}
$$

2. Recursion: for $2\le t \le T$ and $1 \le j \le N$

$$
\begin{matrix}
\delta_j(t)&=&\underset{i}{max}[\delta_i(t-1)a_{ij}]b_j(y_{t})\\\\
\xi_j(1) &=& \underset{1\le i\le N}{argmax}\delta_i(t)a_{ij}
\end{matrix}
\tag{A7}
$$

3. Output:

$$
\begin{matrix}
\delta_j(1) &=& p_jb_j(y_1) \\\\
\xi_j(1) &=& 0
\end{matrix}
\tag{A8}
$$
-->
<p><a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Viterbi algorithm</a> is created to solve question 2. The notations follows <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Wikipedia</a> and the code can be found in <a href="https://github.com/EliteQuant/EliteQuant_Python/blob/master/research/hidden_markov_chain.py" target="_blank" rel="noopener">Github</a>.</p>
<table>
<thead>
<tr>
<th>Day</th>
<th>Prob</th>
<th>Max Sate</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>$T_1(1,1)=0.5\times0.9=0.45$</td>
<td>$T_2(1,1)=’Up’$</td>
</tr>
<tr>
<td></td>
<td>$T_1(2,1)=0.5\times0.4=0.2$</td>
<td>$T_2(2,1)=’Up’$</td>
</tr>
<tr>
<td>2</td>
<td>$T_1(1,2)=max\left(0.45\times0.8,0.2\times0.3\right)\times0.9=0.324$</td>
<td>$T_2(1,2)=’Up’$</td>
</tr>
<tr>
<td></td>
<td>$T_1(2,2)=max\left(0.45\times0.2,0.2\times0.7\right)\times0.4=0.056$</td>
<td>$T_2(2,2)=’Down’$ </td>
</tr>
<tr>
<td>3</td>
<td>$T_1(1,3)=max\left(0.324*\times0.8,0.056\times0.3\right)\times0.9=0.23328$</td>
<td>$T_2(1,3)=’Up’$</td>
</tr>
<tr>
<td></td>
<td>$T_1(2,3)=max\left(0.324\times0.2,0.056\times0.7\right)\times0.4=0.02592$</td>
<td>$T_2(2,3)=’Up’$</td>
</tr>
</tbody>
</table>
<p>On day 1, the table is initialized. Then on day 2 and day3, it uses <a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank" rel="noopener">dynamic programming</a> to find the optimal probability and states recursively. Finally, the most probable hidden states for the three days are {‘Up’,’Up’,’Up’} with maximum probability of $23.328\%$. In other words, if she is happy three days in a row, most likely the market is also on a three-day winning streak.</p>
<h3 id="Third-Question-–-Learning"><a href="#Third-Question-–-Learning" class="headerlink" title="Third Question – Learning"></a>Third Question – Learning</h3><p>The well-known <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener">Baum–Welch algorithm</a> is designed to find the local maximum iteratively. It takes the following steps, </p>
<ol>
<li>Calculate forward probabilities $\alpha_i(t)$ with the forward algorithm</li>
<li>Calculate backward probabilities $\beta_i(t)$ with the backward algorithm</li>
<li>Expectation – calculate the probability of being in state $i$ at time $t$, $\gamma_i(t)$; and the probability of being in state $i$ and $j$ at times $t$ and $t+1$, $\xi_{ij}(t)$</li>
<li>Maximization – update the new model parameters (start probabilities, transition probabilities, emission probabilities)</li>
<li>Repeat step 1 through 4 until the change in log likelihood converges.</li>
</ol>
<p>Back to our example, assume $\theta$ in the figure above is not the actual but our current belief. Then we have done step 1 and 2 as in the first question. To proceed.</p>
<table>
<thead>
<tr>
<th>Day</th>
<th>Probability</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>$\gamma_1(1)=0.790368$</td>
<td>$\gamma_2(1)=0.209632$</td>
</tr>
<tr>
<td></td>
<td>$\xi_{11}(1)=0.2592$</td>
<td>$\xi_{12}(1)=0.0198$</td>
</tr>
<tr>
<td></td>
<td>$\xi_{21}(1)=0.0432$</td>
<td>$\xi_{22}(1)=0.0308$</td>
</tr>
<tr>
<td>2</td>
<td>$\gamma_1(2)=0.856657$</td>
<td>$\gamma_2(2)=0.143343$</td>
</tr>
<tr>
<td></td>
<td>$\xi_{11}(2)=0.27216$</td>
<td>$\xi_{12}(2)=0.03024$</td>
</tr>
<tr>
<td></td>
<td>$\xi_{21}(2)=0.02484$</td>
<td>$\xi_{22}(2)=0.02576$</td>
</tr>
<tr>
<td>3</td>
<td>$\gamma_1(3)=0.84136$</td>
<td>$\gamma_2(3)=0.15864$</td>
</tr>
</tbody>
</table>
<p>Step $4$ updates the new normalized parameters as follows,</p>
<p>$$<br>\begin{matrix}<br>A^{1}=\left[ \begin {array}{cc} 0.913932 &amp; 0.086068 \\ 0.546067 &amp; 0.453933 \end{array} \right], B^{1}=\left[ \begin {array}{cc} 1.0 &amp; 0.0 \\ 1.0 &amp; 0.0 \end{array} \right],\pi^{1}=\left[ \begin {array}{c} 0.790368 \\ 0.209632 \end{array} \right]<br>\tag{A1}<br>\end{matrix}<br>$$</p>
<p>These steps are now repeated iteratively until it converges.</p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Letian Wang
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="https://letianzj.github.io/hidden-markov-chain.html" title="Hidden Markov Chain">https://letianzj.github.io/hidden-markov-chain.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/welcome.html" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/rnn-stock-prediction.html" rel="prev" title="RNN Stock Prediction">
                RNN Stock Prediction <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="Letian Wang" />
            
              <p class="site-author-name" itemprop="name">Letian Wang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/letianzj" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:letian.zj@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.linkedin.com/in/letian-wang-phd-cfa-frm-75b53312/" target="_blank" title="Linkedin">
                      
                        <i class="fa fa-fw fa-linkedin"></i>Linkedin</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regime-Detection"><span class="nav-number">2.</span> <span class="nav-text">Regime Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix"><span class="nav-number">3.</span> <span class="nav-text">Appendix</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#First-Question-–-Evaluation"><span class="nav-number">3.1.</span> <span class="nav-text">First Question – Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Second-Question-–-Decoding"><span class="nav-number">3.2.</span> <span class="nav-text">Second Question – Decoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Third-Question-–-Learning"><span class="nav-number">3.3.</span> <span class="nav-text">Third Question – Learning</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Letian Wang</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" href="http://hexo.io" rel="external nofollow">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next" rel="external nofollow">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-letianzj-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://letianzj.github.io/hidden-markov-chain.html';
          this.page.identifier = 'hidden-markov-chain.html';
          this.page.title = 'Hidden Markov Chain';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-letianzj-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
