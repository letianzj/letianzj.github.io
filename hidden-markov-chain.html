<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="bokhEjod5-cme1ZueY6aAPHORfnXv9hSoljSfg2Vp_Q">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"letianzj.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Introduction Hidden Markov Model (HMM) is a Markov Model with latent state space. It is the discrete version of Dynamic Linear Model, commonly seen in speech recognition. In quantitative trading, it">
<meta property="og:type" content="article">
<meta property="og:title" content="Hidden Markov Chain">
<meta property="og:url" content="https://letianzj.github.io/hidden-markov-chain.html">
<meta property="og:site_name" content="Systematic Investing">
<meta property="og:description" content="Introduction Hidden Markov Model (HMM) is a Markov Model with latent state space. It is the discrete version of Dynamic Linear Model, commonly seen in speech recognition. In quantitative trading, it">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://letianzj.github.io/example.png">
<meta property="og:image" content="https://letianzj.github.io/hidden-markov-chain/example.png">
<meta property="og:image" content="https://letianzj.github.io/hidden-markov-chain/spx_hmm.png">
<meta property="article:published_time" content="2018-09-02T11:05:00.000Z">
<meta property="article:modified_time" content="2020-07-05T19:48:22.439Z">
<meta property="article:author" content="Letian Wang">
<meta property="article:tag" content="Hidden Markov Chain">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Regime Detection">
<meta property="article:tag" content="Backward Algorithm">
<meta property="article:tag" content="Forward Algorithm">
<meta property="article:tag" content="Viterbi Algorithm">
<meta property="article:tag" content="Baum-Welch Algorithm">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://letianzj.github.io/example.png">

<link rel="canonical" href="https://letianzj.github.io/hidden-markov-chain.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Hidden Markov Chain | Systematic Investing</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-109341958-2"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-109341958-2');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Systematic Investing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Quant Trading and Portfolio Management</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://letianzj.github.io/hidden-markov-chain.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Letian Wang">
      <meta itemprop="description" content="Letian Wang quantitative trading blog, discuss topics includding trading strategies, portfolio management, risk premia, risk management, systematic trading, and machine learning, deep learning applications in Finance.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Systematic Investing">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Hidden Markov Chain
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-02 07:05:00" itemprop="dateCreated datePublished" datetime="2018-09-02T07:05:00-04:00">2018-09-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-05 15:48:22" itemprop="dateModified" datetime="2020-07-05T15:48:22-04:00">2020-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/hidden-markov-chain.html#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="hidden-markov-chain.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="introduction">Introduction</h2>
<p><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" target="_blank" rel="noopener">Hidden Markov Model (HMM)</a> is a <a href="https://en.wikipedia.org/wiki/Markov_model" target="_blank" rel="noopener">Markov Model</a> with latent <a href="https://en.wikipedia.org/wiki/State_space" target="_blank" rel="noopener">state space</a>. It is the discrete version of Dynamic Linear Model, commonly seen in speech recognition. In quantitative trading, it has been applied to detecting latent market regimes ([2], [3]). I'll relegate technical details to appendix and present the intuitions by an example.</p>
<a id="more"></a>
<!--<img src="example.png" align="left" width="100%" height="100%">-->
<img src="/hidden-markov-chain/example.png" class="" title="Example">
<p>No cell phone, no TV, without logging into Bloomberg terminal, this simple example teaches you how to guess market movements solely from the trader's mood. Here the two hidden states are market rally or drop. The transition matrix stipulates that if the market goes up today, there are 80% chance that the trend will continue tomorrow and 20% chance that the trend will reverse. On the contrary, if the market drops today, there are 70% chance sliding further tomorrow and 30% chance turning back up.</p>
<p>The emission matrix shows the dependency of the trader's mood on mareket changes. If she is just moody (high noise/information ratio), it would tell us nothing about the market. If somehow her mood is driven by her PnL, then we might be able to learn the market from her behavior. In this example, it says that if the market heads up, she would most likely be happy but there are 10% chance that something else, for example a car accident in the morning, upsets here. On the contrary, if the market goes down she would be unhappy, unless something else, perphas today happens to be her anniversary, cheers her up.</p>
<p>With all these uncertainties/probabilities in place, there are three questions to ask, in the order of complexity:</p>
<ol type="1">
<li><p><strong>Evaluation</strong>: What is the odds that she is happy three days in a row?</p></li>
<li><p><strong>Decoding</strong>: Given that she is happy three days in a row, what would be the most likely market movements in the last three days?</p></li>
<li><p><strong>Learning</strong>: If we don't know any of the probablities labeled next to the arrows in the figure, given that she is happy three days in a row, what would be our best estimates of these probabilities (model parameters)?</p></li>
</ol>
<p>The first question can be answered by <a href="https://en.wikipedia.org/wiki/Forward_algorithm" target="_blank" rel="noopener">forward</a> OR backward algorithm (do not confuse with <a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm" target="_blank" rel="noopener">forward-backward algorithm</a>). The second question is tackled by <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Viterbi algorithm</a>. The third one is solved by <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener">Baum-Welch algorithm</a>, an <a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm" target="_blank" rel="noopener">forward-backward</a> <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" target="_blank" rel="noopener">expectation maximization</a> approach.</p>
<h2 id="regime-detection">Regime Detection</h2>
<p>This section follows the <a href="https://hmmlearn.readthedocs.io/en/latest/auto_examples/plot_hmm_stock_analysis.html" target="_blank" rel="noopener">accompanying example</a> of the package <a href="https://github.com/hmmlearn/hmmlearn" target="_blank" rel="noopener">HMMLearn</a>. The code is located on <a href="https://github.com/letianzj/QuantResearch/blob/master/notebooks/hidden_markov_chain.py" target="_blank" rel="noopener">Github</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hmm_model = GaussianHMM(</span><br><span class="line">    n_components=<span class="number">3</span>,                     <span class="comment"># number of states</span></span><br><span class="line">    covariance_type=<span class="string">"full"</span>,             <span class="comment"># full covariance matrix vs diagonal</span></span><br><span class="line">    n_iter=<span class="number">1000</span>                         <span class="comment"># number of iterations</span></span><br><span class="line">).fit(rets)</span><br><span class="line"></span><br><span class="line">print(hmm_model.transmat_)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Means and vars of each hidden state"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(hmm_model.n_components):                   <span class="comment"># 0 is down, 1 is up</span></span><br><span class="line">    print(<span class="string">"&#123;0&#125;th hidden state"</span>.format(i))</span><br><span class="line">    print(<span class="string">"mean = "</span>, hmm_model.means_[i])</span><br><span class="line">    print(<span class="string">"var = "</span>, np.diag(hmm_model.covars_[i]))</span><br></pre></td></tr></table></figure>
<p>From the mean and standard deviation, it seems that state 0 is market-down state; state 1 and 2 are market-up states. Within them, state 3 has <span class="math inline">\(65\%\)</span> of chance flipping to the downside. Below is the historical states distribution of S&amp;P 500 index. At first glance it doesn't seem to be a very precise result. It might need to be combined with other strategies.</p>
<img src="/hidden-markov-chain/spx_hmm.png" class="" title="HMM States">
<p><strong>Reference</strong> * [1] Petris, Giovanni, Sonia Petrone, and Patrizia Campagnoli. &quot;Dynamic linear models.&quot; Dynamic Linear Models with R. Springer, New York, NY, 2009. 31-84. * [2] Nguyen, Nguyet, and Dung Nguyen. &quot;Hidden Markov model for stock selection.&quot; Risks 3.4 (2015): 455-473. * [3] Nguyen, Nguyet. &quot;Hidden Markov Model for Stock Trading.&quot; International Journal of Financial Studies 6.2 (2018): 36. * [4] Wikipeida, <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" target="_blank" rel="noopener">Hidden Markov Model</a> * [5] Wikipeida, <a href="https://en.wikipedia.org/wiki/Forward_algorithm" target="_blank" rel="noopener">Forward algorithm</a> * [6] Wikipedia, <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Viterbi algorithm</a> * [7] Wikipedia, <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener">Baum–Welch algorithm</a></p>
<h2 id="appendix">Appendix</h2>
<p>You might need some basic understanding about <a href="https://en.wikipedia.org/wiki/Markov_chain" target="_blank" rel="noopener">Markov Chain</a> in order to continue.</p>
<p>This example uses the notations on <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener">Wikipedia</a>. In particular the hidden markov system <span class="math inline">\(\theta=\left(A,B,\pi\right)\)</span> is given by</p>
<p><span class="math display">\[
\begin{matrix}
A=\left[ \begin {array}{cc} 0.8 &amp; 0.2\\\\ 0.3 &amp; 0.7 \end{array} \right], B=\left[ \begin {array}{cc} 0.9 &amp; 0.1\\\\ 0.4 &amp; 0.6 \end{array} \right],\pi=\left[ \begin {array}{c} 0.5\\\\ 0.5 \end{array} \right]
\tag{A1}
\end{matrix}
\]</span></p>
<p>You can find these numbers from the figure above.</p>
<h3 id="first-question----evaluation">First Question -- Evaluation</h3>
<p>The first question is, given we know <span class="math inline">\(\theta\)</span>, what is the odds that she is happy three days in a row? This is answered by the forward procedure or backward procedure and is solved recursively.</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="header">
<th>Day</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(\alpha_1(1)=0.5\times 0.9=0.45\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\alpha_2(1)=0.5\times 0.4=0.2\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(\alpha_1(2)=0.9\times\left[\alpha_1(1)\times0.8+\alpha_2(1)\times0.3\right]=0.378\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\alpha_2(2)=0.4\times\left[\alpha_1(1)\times0.2+\alpha_2(1)\times0.7\right]=0.092\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(\alpha_1(3)=0.9\times\left[\alpha_1(2)\times0.8+\alpha_2(2)\times0.3\right]=0.297\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\alpha_2(3)=0.4\times\left[\alpha_1(2)\times0.2+\alpha_2(2)\times0.7\right]=0.056\)</span></td>
</tr>
</tbody>
</table>
<p>On day 1, there are <span class="math inline">\(45\%\)</span> chance to observe a happy face (H) and market is up; there are <span class="math inline">\(20\%\)</span> chance to observe a happy face (H) and market is down. So in total there are <span class="math inline">\(65\%\)</span> chance that she would be happy and 35% chance that she would be sad (S). Similarly, the probabilty of seeing <span class="math inline">\(HH\)</span> is <span class="math inline">\(47\%\)</span>. Other possibilities (that is, <span class="math inline">\(HH\)</span>, <span class="math inline">\(HS\)</span> and <span class="math inline">\(SS\)</span>) add up to <span class="math inline">\(53\%\)</span>.</p>
<p>Finailly on day three, the answer to our question becomes <span class="math inline">\(35.3\%\)</span>.</p>
<p>If we iterate backwards,</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="header">
<th>Day</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(\beta_1(3)=1.0\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\beta_2(3)=1.0\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(\beta_1(2)=\beta_1(2)\times0.8\times0.9+\beta_2(2)\times0.2\times0.4=0.8\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\beta_2(2)=\beta_1(2)\times0.3\times0.9+\beta_2(2)\times0.7\times0.4=0.55\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(\beta_1(1)=0.9\times[\beta_1(2)\times0.8+\beta_2(2)\times0.3]=0.62\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\beta_2(1)=0.4\times[\beta_1(2)\times0.2+\beta_2(2)\times0.7]=0.37\)</span></td>
</tr>
</tbody>
</table>
<p>The final probability should match that from the forward algorithm ([2])</p>
<p><span class="math display">\[
Pr(HHH|\theta)=0.5\times0.62\times0.9+0.5\times0.37\times0.4=35.3\%
\tag{A2}
\]</span></p>
<h3 id="second-question----decoding">Second Question -- Decoding</h3>
<!--
Let define ([2])

$$
\delta_i(t)=\underset{x_1,x_2,...,x_{t-1}}{max}P\left(X_1=x_1,X_2=x_2,...,X_t=i,Y_1=y_1,...,Y_t=y_t|\theta \right)
\tag{A3}
$$

By induction, we have:

$$
\delta_j(t+1)=\underset{i}{max}[\delta_i(t)a_{ij}]b_j(y_{t+1})
\tag{A4}
$$

The most likely state $i$, at time $t$, is the one that maximize (A3):

$$
i_t=\underset{1\le i\le N}{argmax}\delta_i(t)
\tag{A5}
$$

Therefore, the Viterbi algorithm is given by

1. Initialization:

$$
\begin{matrix}
\delta_j(1) &=& p_jb_j(y_1) \\\\
\xi_j(1) &=& 0
\end{matrix}
\tag{A6}
$$

2. Recursion: for $2\le t \le T$ and $1 \le j \le N$

$$
\begin{matrix}
\delta_j(t)&=&\underset{i}{max}[\delta_i(t-1)a_{ij}]b_j(y_{t})\\\\
\xi_j(1) &=& \underset{1\le i\le N}{argmax}\delta_i(t)a_{ij}
\end{matrix}
\tag{A7}
$$

3. Output:

$$
\begin{matrix}
\delta_j(1) &=& p_jb_j(y_1) \\\\
\xi_j(1) &=& 0
\end{matrix}
\tag{A8}
$$
-->
<p><a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Viterbi algorithm</a> is created to solve question 2. The notations follows <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Wikipedia</a> and the code can be found in <a href="https://github.com/letianzj/QuantResearch/blob/master/notebooks/hidden_markov_chain.py" target="_blank" rel="noopener">Github</a>.</p>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 46%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="header">
<th>Day</th>
<th>Prob</th>
<th>Max Sate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(T_1(1,1)=0.5\times0.9=0.45\)</span></td>
<td><span class="math inline">\(T_2(1,1)=&#39;Up&#39;\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(T_1(2,1)=0.5\times0.4=0.2\)</span></td>
<td><span class="math inline">\(T_2(2,1)=&#39;Up&#39;\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(T_1(1,2)=max\left(0.45\times0.8,0.2\times0.3\right)\times0.9=0.324\)</span></td>
<td><span class="math inline">\(T_2(1,2)=&#39;Up&#39;\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(T_1(2,2)=max\left(0.45\times0.2,0.2\times0.7\right)\times0.4=0.056\)</span></td>
<td><span class="math inline">\(T_2(2,2)=&#39;Down&#39;\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(T_1(1,3)=max\left(0.324*\times0.8,0.056\times0.3\right)\times0.9=0.23328\)</span></td>
<td><span class="math inline">\(T_2(1,3)=&#39;Up&#39;\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(T_1(2,3)=max\left(0.324\times0.2,0.056\times0.7\right)\times0.4=0.02592\)</span></td>
<td><span class="math inline">\(T_2(2,3)=&#39;Up&#39;\)</span></td>
</tr>
</tbody>
</table>
<p>On day 1, the table is initialized. Then on day 2 and day3, it uses <a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank" rel="noopener">dynamic programming</a> to find the optimal probability and states recursively. Finally, the most probable hidden states for the three days are {'Up','Up','Up'} with maximum probability of <span class="math inline">\(23.328\%\)</span>. In other words, if she is happy three days in a row, most likely the market is also on a three-day winning streak.</p>
<h3 id="third-question----learning">Third Question -- Learning</h3>
<p>The well-known <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank" rel="noopener">Baum–Welch algorithm</a> is designed to find the local maximum iteratively. It takes the following steps,</p>
<ol type="1">
<li>Calculate forward probabilities <span class="math inline">\(\alpha_i(t)\)</span> with the forward algorithm</li>
<li>Calculate backward probabilities <span class="math inline">\(\beta_i(t)\)</span> with the backward algorithm</li>
<li>Expectation -- calculate the probability of being in state <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\gamma_i(t)\)</span>; and the probability of being in state <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> at times <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span>, <span class="math inline">\(\xi_{ij}(t)\)</span></li>
<li>Maximization -- update the new model parameters (start probabilities, transition probabilities, emission probabilities)</li>
<li>Repeat step 1 through 4 until the change in log likelihood converges.</li>
</ol>
<p>Back to our example, assume <span class="math inline">\(\theta\)</span> in the figure above is not the actual but our current belief. Then we have done step 1 and 2 as in the first question. To proceed.</p>
<table>
<thead>
<tr class="header">
<th>Day</th>
<th>Probability</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(\gamma_1(1)=0.790368\)</span></td>
<td><span class="math inline">\(\gamma_2(1)=0.209632\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\xi_{11}(1)=0.2592\)</span></td>
<td><span class="math inline">\(\xi_{12}(1)=0.0198\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td><span class="math inline">\(\xi_{21}(1)=0.0432\)</span></td>
<td><span class="math inline">\(\xi_{22}(1)=0.0308\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(\gamma_1(2)=0.856657\)</span></td>
<td><span class="math inline">\(\gamma_2(2)=0.143343\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td><span class="math inline">\(\xi_{11}(2)=0.27216\)</span></td>
<td><span class="math inline">\(\xi_{12}(2)=0.03024\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\xi_{21}(2)=0.02484\)</span></td>
<td><span class="math inline">\(\xi_{22}(2)=0.02576\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(\gamma_1(3)=0.84136\)</span></td>
<td><span class="math inline">\(\gamma_2(3)=0.15864\)</span></td>
</tr>
</tbody>
</table>
<p>Step <span class="math inline">\(4\)</span> updates the new normalized parameters as follows,</p>
<p><span class="math display">\[
\begin{matrix}
A^{1}=\left[ \begin {array}{cc} 0.913932 &amp; 0.086068 \\\\ 0.546067 &amp; 0.453933 \end{array} \right], B^{1}=\left[ \begin {array}{cc} 1.0 &amp; 0.0 \\\\ 1.0 &amp; 0.0 \end{array} \right],\pi^{1}=\left[ \begin {array}{c} 0.790368 \\\\ 0.209632 \end{array} \right]
\tag{A1}
\end{matrix}
\]</span></p>
<p>These steps are now repeated iteratively until it converges.</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Letian Wang
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://letianzj.github.io/hidden-markov-chain.html" title="Hidden Markov Chain">https://letianzj.github.io/hidden-markov-chain.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/kalman-filter-pairs-trading.html" rel="prev" title="Kalman Filter and Pairs Trading">
      <i class="fa fa-chevron-left"></i> Kalman Filter and Pairs Trading
    </a></div>
      <div class="post-nav-item">
    <a href="/rnn-stock-prediction.html" rel="next" title="RNN Stock Prediction">
      RNN Stock Prediction <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regime-detection"><span class="nav-number">2.</span> <span class="nav-text">Regime Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#appendix"><span class="nav-number">3.</span> <span class="nav-text">Appendix</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#first-question----evaluation"><span class="nav-number">3.1.</span> <span class="nav-text">First Question -- Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#second-question----decoding"><span class="nav-number">3.2.</span> <span class="nav-text">Second Question -- Decoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#third-question----learning"><span class="nav-number">3.3.</span> <span class="nav-text">Third Question -- Learning</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Letian Wang"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Letian Wang</p>
  <div class="site-description" itemprop="description">Letian Wang quantitative trading blog, discuss topics includding trading strategies, portfolio management, risk premia, risk management, systematic trading, and machine learning, deep learning applications in Finance.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/letianzj" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;letianzj" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:letian.zj@gmail.com" title="E-Mail → mailto:letian.zj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/letian-wang-phd-cfa-frm-75b53312/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;letian-wang-phd-cfa-frm-75b53312&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Letian Wang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-letianzj-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://letianzj.github.io/hidden-markov-chain.html";
    this.page.identifier = "hidden-markov-chain.html";
    this.page.title = "Hidden Markov Chain";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://https-letianzj-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
